{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5211369",
   "metadata": {},
   "outputs": [],
   "source": [
    "各个模型原理必须熟知，后面的专项才能归纳到位！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28f896",
   "metadata": {},
   "source": [
    "机器学习\n",
    "\n",
    "吴恩达机器学习\n",
    "\n",
    "- 机器学习属于module learning，有一个大的假设空间，search the best假设用来解释这些数据，这个假设的表现形式就是一个函数或者说数学模型，可以用来预测(回归)或者分类\n",
    "- 分成几个大类\n",
    "   - reinforcement Learning \n",
    "     - 对结果带有奖惩机制\n",
    "     - 比如下棋\n",
    "   - supervised learning\n",
    "     - 线性模型\n",
    "       - 找寻feature和label的关系，可能是线性函数(y=ax+b)，可能是高阶线性关系\n",
    "       - 如何评价呢？loss fun越小越好\n",
    "       - 如何求解呢？梯度下降，如果数据量小直接LMS 线性奇异值分解\n",
    "         - 梯度下降呢，又有mini batch，随机梯度下降，学习率变化的梯度下降\n",
    "         - 跳出局部最优解呢\n",
    "             - 调整学习率\n",
    "             - 遗传算法的化，通过变异。\n",
    "     - svm\n",
    "   - unsupervised learing\n",
    "\n",
    "- 不同的模型都是这么个顺序\n",
    "   - module\n",
    "   - loss fun\n",
    "   - 求解loss最小，如何求呢最优值呢？梯度下降\n",
    "     - 求导，求微分\n",
    "     - \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba84e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "感知机\n",
    "SVM\n",
    "树模型\n",
    "集成模型\n",
    "PCA\n",
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738fcf6",
   "metadata": {},
   "source": [
    "# 专题：归一化&标准化&中心化/0-均值化\n",
    "\n",
    "- 本质就是数据缩放，都是一种数据的线性变化\n",
    "  - 线性变化不改变原始数据的数值排序\n",
    "\n",
    "- 为什么需要变换\n",
    "   - 减少计算量\n",
    "   - 加速迭代，未归一化/标准化时形成的等高线偏椭圆，迭代时很有可能走“之”字型路线（垂直长轴），从而导致迭代很多次才能收敛，标准化之后，等高线变成圆，加快收敛。\n",
    "   - 避免数据范围(量纲)对结果产生的影响，比如分类的时候计算距离，个别特征数值太大。比如房价和面积，量纲不同，可以归一化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783ed56",
   "metadata": {},
   "source": [
    "- 归一化 normalization\n",
    "  - $ x = (x-x_min)/(x_max - x_min) $\n",
    "  - 看中的是把数据缩放到0-1区间，拍平\n",
    "- 标准化 z-score standardization\n",
    "  - $ x = (x - u)/ \\alpha $\n",
    "  - 关注每一个x，关注数据的分布，标准化之后不是0-1区间\n",
    "- 中心化/0-均值化 zero-centered\n",
    "  - $ x = (x - u)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a664cc",
   "metadata": {},
   "source": [
    "- 什么时候用归一化？什么时候用标准化\n",
    "   - 如果对数据输出范围有要求，或者数据比较稳定，没有极端的最大最小值，用归一化\n",
    "   - 如果关注数据本身的分布，数据存在异常值或噪声值，用标准化，可以间接通过中心化避免异常值和极端值的影响\n",
    "   - 按模型来说\n",
    "     - svm：因为不同模型的特征分布假设不同，比如svm使用高斯核的时候，所有维度公用一个方差\n",
    "     - KNN：需要度量距离、协方差计算的模型，一般需要归一化/标准化，不然会出现大数吃小数\n",
    "     - 神经网络：数值问题、梯度求解需要、学习率\n",
    "     - PCA 降维的时候Z-score standardization更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71b245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
